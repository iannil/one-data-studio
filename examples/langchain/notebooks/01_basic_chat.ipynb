{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE-DATA-STUDIO 基础对话示例\n",
    "\n",
    "本 Notebook 演示如何使用 LangChain 与 ONE-DATA-STUDIO (Bisheng) 的 LLM 服务进行交互。\n",
    "\n",
    "## 前置条件\n",
    "\n",
    "1. ONE-DATA-STUDIO 服务已启动\n",
    "2. 已部署可用的 LLM 模型\n",
    "3. 安装必要的 Python 包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "# !pip install langchain langchain-core requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 配置连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 添加父目录到路径\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# 配置 API 连接\n",
    "BISHENG_API_BASE = os.getenv(\"BISHENG_API_BASE\", \"http://localhost:8000\")\n",
    "BISHENG_API_KEY = os.getenv(\"BISHENG_API_KEY\", \"\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen-7b-chat\")\n",
    "\n",
    "print(f\"API Base: {BISHENG_API_BASE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建 LLM 实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisheng_llm import BishengLLM\n",
    "\n",
    "# 创建 LLM 实例\n",
    "llm = BishengLLM(\n",
    "    api_base=BISHENG_API_BASE,\n",
    "    model_name=MODEL_NAME,\n",
    "    api_key=BISHENG_API_KEY,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "print(f\"LLM Type: {llm._llm_type}\")\n",
    "print(f\"Identifying Params: {llm._identifying_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 基础调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单问答\n",
    "question = \"什么是机器学习？请用简单的语言解释。\"\n",
    "response = llm.invoke(question)\n",
    "print(f\"问题: {question}\")\n",
    "print(f\"\\n回答: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用 PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 创建 Prompt 模板\n",
    "template = \"\"\"你是一位专业的技术专家。请回答以下关于 {topic} 的问题：\n",
    "\n",
    "问题：{question}\n",
    "\n",
    "请提供详细且易于理解的回答：\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# 格式化 Prompt\n",
    "formatted_prompt = prompt.format(\n",
    "    topic=\"Python 编程\",\n",
    "    question=\"如何高效地处理大文件？\"\n",
    ")\n",
    "\n",
    "print(\"Formatted Prompt:\")\n",
    "print(formatted_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 调用 LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 使用 LCEL (LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 创建链\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 执行链\n",
    "result = chain.invoke({\n",
    "    \"topic\": \"数据库优化\",\n",
    "    \"question\": \"如何优化 SQL 查询性能？\"\n",
    "})\n",
    "\n",
    "print(\"Chain Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 多轮对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# 创建对话记忆\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 创建对话链\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一轮对话\n",
    "response1 = conversation.predict(input=\"你好！我想学习 Python。\")\n",
    "print(f\"AI: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二轮对话 (会记住上下文)\n",
    "response2 = conversation.predict(input=\"我应该从哪里开始？\")\n",
    "print(f\"AI: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三轮对话\n",
    "response3 = conversation.predict(input=\"有哪些推荐的学习资源？\")\n",
    "print(f\"AI: {response3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看对话历史\n",
    "print(\"\\n对话历史:\")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建支持流式的 LLM\n",
    "streaming_llm = BishengLLM(\n",
    "    api_base=BISHENG_API_BASE,\n",
    "    model_name=MODEL_NAME,\n",
    "    api_key=BISHENG_API_KEY,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# 流式输出\n",
    "print(\"流式输出演示:\")\n",
    "for chunk in streaming_llm.stream(\"请写一首关于春天的短诗。\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()  # 换行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 批量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量问题\n",
    "questions = [\n",
    "    \"Python 的优点是什么？\",\n",
    "    \"JavaScript 和 Python 的区别？\",\n",
    "    \"如何选择编程语言？\"\n",
    "]\n",
    "\n",
    "# 批量调用\n",
    "responses = llm.batch(questions)\n",
    "\n",
    "for q, r in zip(questions, responses):\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {r[:200]}...\\n\")  # 只显示前200字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本 Notebook 演示了：\n",
    "\n",
    "1. **基础配置** - 如何配置 Bisheng LLM 连接\n",
    "2. **简单调用** - 直接调用 LLM 进行问答\n",
    "3. **Prompt 模板** - 使用 LangChain PromptTemplate 格式化输入\n",
    "4. **LCEL 链** - 使用 LangChain Expression Language 构建处理链\n",
    "5. **多轮对话** - 使用 Memory 实现上下文记忆\n",
    "6. **流式输出** - 实时获取生成结果\n",
    "7. **批量处理** - 同时处理多个请求\n",
    "\n",
    "更多高级用法请参考 RAG Pipeline 示例。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
