{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE-DATA-STUDIO RAG Pipeline 示例\n",
    "\n",
    "本 Notebook 演示如何使用 LangChain 与 ONE-DATA-STUDIO (Bisheng) 构建完整的 RAG (Retrieval-Augmented Generation) 流水线。\n",
    "\n",
    "## 架构概览\n",
    "\n",
    "```\n",
    "文档 → 分块 → 向量化 → Milvus 存储\n",
    "                              ↓\n",
    "查询 → 向量检索 → 上下文增强 → LLM 生成 → 回答\n",
    "```\n",
    "\n",
    "## 前置条件\n",
    "\n",
    "1. ONE-DATA-STUDIO 服务已启动\n",
    "2. Milvus 向量数据库可用\n",
    "3. 已部署 LLM 和 Embedding 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "# !pip install langchain langchain-core langchain-openai requests tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 配置连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 添加父目录到路径\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# 配置\n",
    "BISHENG_API_BASE = os.getenv(\"BISHENG_API_BASE\", \"http://localhost:8000\")\n",
    "BISHENG_API_KEY = os.getenv(\"BISHENG_API_KEY\", \"\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen-7b-chat\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"bge-large-zh\")\n",
    "COLLECTION_NAME = \"demo_rag_collection\"\n",
    "\n",
    "print(f\"API Base: {BISHENG_API_BASE}\")\n",
    "print(f\"LLM Model: {MODEL_NAME}\")\n",
    "print(f\"Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 初始化组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisheng_llm import BishengLLM\n",
    "from bisheng_vectorstore import BishengVectorStore\n",
    "\n",
    "# 创建 LLM\n",
    "llm = BishengLLM(\n",
    "    api_base=BISHENG_API_BASE,\n",
    "    model_name=MODEL_NAME,\n",
    "    api_key=BISHENG_API_KEY,\n",
    "    temperature=0.3,  # RAG 场景使用较低温度\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "# 创建向量存储\n",
    "vectorstore = BishengVectorStore(\n",
    "    api_base=BISHENG_API_BASE,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    api_key=BISHENG_API_KEY,\n",
    ")\n",
    "\n",
    "print(\"Components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准备示例文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例文档 - ONE-DATA-STUDIO 介绍\n",
    "documents = [\n",
    "    {\n",
    "        \"content\": \"\"\"ONE-DATA-STUDIO 是一个企业级数据 + AI + LLM 融合平台。\n",
    "它整合了三个核心平台：Alldata（数据治理）、Cube Studio（MLOps）和 Bisheng（LLMOps）。\n",
    "平台采用四层架构设计，从下到上分别是：基础设施层、数据底座层、算法引擎层和应用编排层。\"\"\",\n",
    "        \"metadata\": {\"source\": \"overview\", \"section\": \"introduction\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Alldata 是数据治理与开发平台，负责数据集成、ETL、数据治理、特征存储和向量存储。\n",
    "它支持多种数据源接入，包括 MySQL、PostgreSQL、Hive、Kafka 等。\n",
    "Alldata 的元数据管理功能可以自动发现数据资产，追踪数据血缘关系。\"\"\",\n",
    "        \"metadata\": {\"source\": \"alldata\", \"section\": \"features\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Cube Studio 是云原生 MLOps 平台，提供 Notebook 开发、分布式训练和模型服务能力。\n",
    "它支持 TensorFlow、PyTorch、Hugging Face 等主流框架。\n",
    "模型服务采用 vLLM 或 TGI 进行部署，通过 OpenAI 兼容 API 对外暴露。\"\"\",\n",
    "        \"metadata\": {\"source\": \"cube_studio\", \"section\": \"features\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Bisheng 是大模型应用开发平台，专注于 RAG 流水线、Agent 编排和 Prompt 管理。\n",
    "它提供可视化工作流编辑器，支持拖拽式构建复杂的 AI 应用。\n",
    "Bisheng 内置了多种节点类型：LLM 调用、向量检索、条件分支、循环、Agent 等。\"\"\",\n",
    "        \"metadata\": {\"source\": \"bisheng\", \"section\": \"features\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Text-to-SQL 是 ONE-DATA-STUDIO 的核心功能之一。\n",
    "它将 Alldata 的元数据（表结构、字段说明、关系）注入到 Prompt 中，\n",
    "让 LLM 能够根据用户的自然语言查询生成准确的 SQL 语句。\n",
    "系统还支持 SQL 解释和结果可视化。\"\"\",\n",
    "        \"metadata\": {\"source\": \"text2sql\", \"section\": \"features\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"ONE-DATA-STUDIO 的工作流节点包括：\n",
    "- 输入/输出节点：定义工作流的入口和出口\n",
    "- LLM 节点：调用大语言模型生成内容\n",
    "- 检索节点：从向量数据库检索相关文档\n",
    "- 条件节点：根据条件进行分支处理\n",
    "- Agent 节点：自主决策和工具调用\n",
    "- 并行节点：同时执行多个分支\"\"\",\n",
    "        \"metadata\": {\"source\": \"workflow\", \"section\": \"nodes\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"平台支持多种 Agent 工具：\n",
    "- WebBrowserTool：网页内容抓取\n",
    "- FileReaderTool：读取 CSV、Excel、JSON 文件\n",
    "- CodeExecutorTool：安全沙箱中执行 Python 代码\n",
    "- NotificationTool：发送 Slack、钉钉通知\n",
    "所有工具都有安全限制，如 URL 白名单、执行超时等。\"\"\",\n",
    "        \"metadata\": {\"source\": \"agent\", \"section\": \"tools\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"部署架构采用 Kubernetes 容器编排。\n",
    "数据库使用 MySQL 主从复制 + ProxySQL 实现读写分离。\n",
    "缓存使用 Redis Sentinel 模式保证高可用。\n",
    "向量数据库使用 Milvus，支持十亿级向量检索。\n",
    "服务通过 Istio 进行流量管理和服务发现。\"\"\",\n",
    "        \"metadata\": {\"source\": \"deployment\", \"section\": \"infrastructure\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 文档向量化与存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 创建文档分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"，\", \" \"]\n",
    ")\n",
    "\n",
    "# 转换为 LangChain Document\n",
    "langchain_docs = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text(doc[\"content\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        langchain_docs.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={**doc[\"metadata\"], \"chunk_id\": i}\n",
    "        ))\n",
    "\n",
    "print(f\"Split into {len(langchain_docs)} chunks\")\n",
    "\n",
    "# 显示前几个 chunk\n",
    "for i, doc in enumerate(langchain_docs[:3]):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加文档到向量存储\n",
    "texts = [doc.page_content for doc in langchain_docs]\n",
    "metadatas = [doc.metadata for doc in langchain_docs]\n",
    "\n",
    "ids = vectorstore.add_texts(texts, metadatas)\n",
    "print(f\"Added {len(ids)} documents to vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 相似性搜索测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试相似性搜索\n",
    "query = \"什么是 Text-to-SQL?\"\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 构建 RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG Prompt 模板\n",
    "RAG_TEMPLATE = \"\"\"你是 ONE-DATA-STUDIO 平台的智能助手。请根据以下上下文信息回答用户的问题。\n",
    "\n",
    "上下文信息：\n",
    "{context}\n",
    "\n",
    "用户问题：{question}\n",
    "\n",
    "请基于上下文信息提供准确、有帮助的回答。如果上下文中没有相关信息，请诚实地说明。\n",
    "\n",
    "回答：\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "# 创建检索器\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# 格式化文档函数\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 构建 RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG Chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RAG 问答测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 RAG 问答\n",
    "questions = [\n",
    "    \"ONE-DATA-STUDIO 是什么平台？\",\n",
    "    \"平台支持哪些 Agent 工具？\",\n",
    "    \"如何部署 ONE-DATA-STUDIO？\",\n",
    "    \"Bisheng 提供了哪些工作流节点？\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 带来源的 RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# 带来源的 RAG Chain\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(\n",
    "    answer=lambda x: llm.invoke(\n",
    "        prompt.format(\n",
    "            context=format_docs(x[\"context\"]),\n",
    "            question=x[\"question\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# 测试\n",
    "question = \"Cube Studio 支持哪些机器学习框架？\"\n",
    "result = rag_chain_with_source.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {result['answer']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for doc in result['context']:\n",
    "    print(f\"  - {doc.metadata.get('source', 'unknown')}: {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 对话式 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 对话式 RAG Prompt\n",
    "CONVERSATIONAL_RAG_TEMPLATE = \"\"\"你是 ONE-DATA-STUDIO 平台的智能助手。\n",
    "\n",
    "对话历史：\n",
    "{chat_history}\n",
    "\n",
    "上下文信息：\n",
    "{context}\n",
    "\n",
    "用户问题：{question}\n",
    "\n",
    "请基于对话历史和上下文信息，提供准确、连贯的回答：\"\"\"\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_template(CONVERSATIONAL_RAG_TEMPLATE)\n",
    "\n",
    "# 对话记忆 (保留最近 5 轮)\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5,\n",
    "    return_messages=False,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "def conversational_rag(question: str) -> str:\n",
    "    # 检索相关文档\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # 获取对话历史\n",
    "    chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "    \n",
    "    # 生成回答\n",
    "    formatted_prompt = conversational_prompt.format(\n",
    "        chat_history=chat_history,\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    answer = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # 保存到记忆\n",
    "    memory.save_context({\"input\": question}, {\"output\": answer})\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多轮对话测试\n",
    "conversations = [\n",
    "    \"ONE-DATA-STUDIO 的架构是什么样的？\",\n",
    "    \"其中的数据层是哪个组件？\",\n",
    "    \"它支持哪些数据源？\",\n",
    "    \"如何与 Bisheng 集成？\"\n",
    "]\n",
    "\n",
    "for q in conversations:\n",
    "    print(f\"\\nUser: {q}\")\n",
    "    answer = conversational_rag(q)\n",
    "    print(f\"Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 清理资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可选：删除测试集合中的数据\n",
    "# vectorstore.delete(filter={\"source\": {\"$exists\": True}})\n",
    "# print(\"Cleaned up test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本 Notebook 演示了完整的 RAG 流水线：\n",
    "\n",
    "1. **组件初始化** - 配置 LLM 和 VectorStore\n",
    "2. **文档处理** - 分块和向量化\n",
    "3. **相似性搜索** - 测试检索效果\n",
    "4. **RAG Chain** - 构建检索增强生成链\n",
    "5. **带来源 RAG** - 返回答案和引用来源\n",
    "6. **对话式 RAG** - 支持多轮对话的 RAG\n",
    "\n",
    "### 最佳实践\n",
    "\n",
    "- **分块策略**：根据文档类型选择合适的分块大小和重叠\n",
    "- **检索数量**：通常 3-5 个相关文档效果较好\n",
    "- **温度设置**：RAG 场景建议使用较低温度 (0.1-0.3)\n",
    "- **Prompt 优化**：明确指示 LLM 基于上下文回答\n",
    "- **结果评估**：定期评估检索准确率和回答质量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
