# 数据处理测试规范

> 功能数: 52 | 模块数: 5
> 领域: 二、数据处理

本文档定义数据处理领域所有功能的测试规范，包含 ETL 批量处理、实时计算 (Flink)、Streaming IDE、离线处理、Kafka 流处理五个模块。

---

## 目录

- [2.1 ETL 批量处理 (ETL)](#21-etl-批量处理-etl)
- [2.2 实时计算 Flink (FLINK)](#22-实时计算-flink-flink)
- [2.3 Streaming IDE (SIDE)](#23-streaming-ide-side)
- [2.4 离线处理 (OFF)](#24-离线处理-off)
- [2.5 Kafka 流处理 (KFK)](#25-kafka-流处理-kfk)

---

## 2.1 ETL 批量处理 (ETL)

**后端代码**: `services/data-api/models/etl.py`
**前端代码**: `web/src/pages/data/etl/ETLPage.tsx`, `web/src/pages/data/etl/KettlePage.tsx`

### ETL-001 ETL 任务创建

**实现目标**: 创建 ETL 同步任务，配置数据抽取、转换、加载的完整流程

**测试步骤**:
1. 进入 ETL 管理页面，点击"创建任务"
2. 填写任务名称、描述
3. 选择任务类型（batch/incremental/cdc/full_sync）
4. 配置源端连接和查询
5. 配置目标端表和字段映射
6. 配置转换规则
7. 保存任务

**成功条件**:
- 任务创建成功
- 配置信息完整保存
- 支持所有任务类型
- 字段映射正确

**失败条件**:
- 任务创建失败
- 配置信息丢失
- 不支持某种任务类型
- 映射关系保存错误

---

### ETL-002 ETL 任务列表

**实现目标**: 分页查询 ETL 任务列表，展示任务状态、执行情况等信息

**测试步骤**:
1. 进入 ETL 管理页面
2. 验证列表显示所有任务
3. 验证显示任务名称、类型、状态、最后执行时间
4. 测试按状态筛选
5. 测试按类型筛选
6. 测试分页功能
7. 测试搜索功能

**成功条件**:
- 列表完整显示所有任务
- 筛选功能正常
- 分页功能正常
- 搜索结果准确

**失败条件**:
- 列表加载失败
- 筛选无效
- 分页不正常
- 搜索结果不准确

---

### ETL-003 ETL 任务编辑

**实现目标**: 修改 ETL 任务的配置，包括源端、目标端、转换规则等

**测试步骤**:
1. 在任务列表选择一个任务
2. 点击"编辑"进入编辑页面
3. 验证原有配置正确加载
4. 修改源端查询 SQL
5. 修改字段映射
6. 修改转换规则
7. 保存修改

**成功条件**:
- 原配置完整加载
- 修改保存成功
- 修改后配置生效
- 版本号更新

**失败条件**:
- 原配置加载不完整
- 修改保存失败
- 修改不生效
- 历史配置被覆盖丢失

---

### ETL-004 ETL 任务删除

**实现目标**: 删除 ETL 任务及其执行历史

**测试步骤**:
1. 选择一个未运行的任务
2. 点击"删除"按钮
3. 确认删除
4. 验证任务从列表移除
5. 验证执行历史被清理
6. 尝试删除运行中的任务

**成功条件**:
- 任务删除成功
- 执行历史同步清理
- 运行中任务提示需先停止
- 删除操作记录日志

**失败条件**:
- 删除失败
- 执行历史残留
- 运行中任务被强制删除
- 无确认步骤

---

### ETL-005 ETL 任务执行

**实现目标**: 手动触发 ETL 任务执行，完成数据同步

**测试步骤**:
1. 选择一个已配置的 ETL 任务
2. 点击"执行"按钮
3. 验证任务状态变为 running
4. 等待执行完成
5. 验证目标端数据正确写入
6. 验证执行日志生成

**成功条件**:
- 任务成功执行
- 数据正确同步
- 状态实时更新
- 日志完整记录

**失败条件**:
- 任务执行失败
- 数据未同步或不完整
- 状态不更新
- 无执行日志

---

### ETL-006 ETL 任务调度

**实现目标**: 配置 ETL 任务的定时调度，支持 cron 表达式和时间间隔

**测试步骤**:
1. 编辑 ETL 任务，进入调度配置
2. 配置 cron 表达式（如每天凌晨 2 点）
3. 保存并启用调度
4. 验证调度任务注册成功
5. 测试 interval 方式（如每 30 分钟）
6. 验证按时自动触发

**成功条件**:
- 调度配置保存成功
- cron 和 interval 都支持
- 任务按时触发
- 调度状态正确显示

**失败条件**:
- 调度配置失败
- cron 表达式解析错误
- 任务未按时触发
- 调度状态不准确

---

### ETL-007 执行日志查询

**实现目标**: 查看 ETL 任务的执行历史日志，支持问题排查

**测试步骤**:
1. 进入任务详情的执行历史页签
2. 验证显示历史执行记录列表
3. 点击某条记录查看详细日志
4. 验证显示开始时间、结束时间、状态
5. 验证显示处理行数统计
6. 验证显示错误信息（如有）
7. 测试日志下载功能

**成功条件**:
- 历史记录完整
- 详细日志可查看
- 统计数据准确
- 错误信息清晰

**失败条件**:
- 历史记录不完整
- 日志加载失败
- 统计数据错误
- 错误信息不可读

---

### ETL-008 源配置管理

**实现目标**: 配置 ETL 任务的源端连接和数据提取方式

**测试步骤**:
1. 创建 ETL 任务，进入源配置
2. 选择源数据源
3. 选择提取方式（表/SQL）
4. 配置表名或编写 SQL
5. 测试 SQL 预览功能
6. 配置增量字段（如 update_time）
7. 保存配置

**成功条件**:
- 数据源列表正确加载
- 表选择和 SQL 编写都支持
- SQL 预览返回数据
- 增量配置正确

**失败条件**:
- 数据源加载失败
- 只支持单一提取方式
- SQL 预览失败
- 增量字段配置无效

---

### ETL-009 目标配置管理

**实现目标**: 配置 ETL 任务的目标端表和写入策略

**测试步骤**:
1. 进入目标配置
2. 选择目标数据源
3. 选择或新建目标表
4. 配置字段映射
5. 配置写入策略（insert/upsert/replace）
6. 配置主键字段
7. 保存配置

**成功条件**:
- 目标数据源正确加载
- 支持选择已有表和新建表
- 字段映射正确
- 写入策略生效

**失败条件**:
- 目标数据源加载失败
- 无法新建目标表
- 字段映射错误
- 写入策略不生效

---

### ETL-010 转换规则配置

**实现目标**: 配置字段级别的数据转换规则

**测试步骤**:
1. 进入转换规则配置
2. 选择一个字段添加转换
3. 配置类型转换（如 string → int）
4. 配置函数转换（如 UPPER、TRIM）
5. 配置表达式转换
6. 配置默认值
7. 测试转换预览

**成功条件**:
- 类型转换正确
- 函数转换正确
- 表达式转换正确
- 预览结果准确

**失败条件**:
- 类型转换失败
- 函数转换错误
- 表达式解析失败
- 预览与实际不符

---

### ETL-011 Kettle 引擎集成

**实现目标**: 支持使用 Kettle/PDI 作为 ETL 执行引擎

**测试步骤**:
1. 创建 ETL 任务，选择 Kettle 引擎
2. 上传或选择 Kettle 作业文件（.kjb/.ktr）
3. 配置作业参数
4. 执行任务
5. 验证 Kettle 日志输出
6. 验证执行结果

**成功条件**:
- Kettle 引擎可选择
- 作业文件正确解析
- 参数传递正确
- 执行成功

**失败条件**:
- Kettle 引擎不可用
- 作业文件解析失败
- 参数传递错误
- 执行失败

---

### ETL-012 Hop 引擎集成

**实现目标**: 支持使用 Apache Hop 作为 ETL 执行引擎

**测试步骤**:
1. 创建 ETL 任务，选择 Hop 引擎
2. 上传或选择 Hop Pipeline 文件
3. 配置 Pipeline 参数
4. 执行任务
5. 验证 Hop 日志输出
6. 验证执行结果

**成功条件**:
- Hop 引擎可选择
- Pipeline 文件正确解析
- 执行成功
- 日志完整

**失败条件**:
- Hop 引擎不可用
- Pipeline 解析失败
- 执行失败
- 日志不完整

---

### ETL-013 Spark 引擎集成

**实现目标**: 支持使用 Spark 作为 ETL 执行引擎处理大数据量

**测试步骤**:
1. 创建 ETL 任务，选择 Spark 引擎
2. 配置 Spark 资源参数
3. 编写或配置 Spark SQL
4. 提交到集群执行
5. 查看 Spark UI 监控
6. 验证执行结果

**成功条件**:
- Spark 引擎可选择
- 资源配置生效
- 任务正确提交到集群
- 大数据量处理成功

**失败条件**:
- Spark 引擎不可用
- 资源配置无效
- 集群提交失败
- 大数据处理失败或超时

---

### ETL-014 Flink 引擎集成

**实现目标**: 支持使用 Flink 作为 ETL 执行引擎

**测试步骤**:
1. 创建 ETL 任务，选择 Flink 引擎
2. 配置 Flink 资源参数
3. 编写 Flink SQL
4. 提交任务执行
5. 查看 Flink Dashboard
6. 验证执行结果

**成功条件**:
- Flink 引擎可选择
- 资源配置生效
- 任务正确执行
- 与 Flink Dashboard 联动

**失败条件**:
- Flink 引擎不可用
- 资源配置无效
- 任务执行失败
- 无法查看监控

---

### ETL-015 AI 字段映射

**实现目标**: AI 自动分析源和目标字段，推荐最佳映射关系

**测试步骤**:
1. 创建 ETL 任务，配置源端和目标端
2. 点击"AI 智能映射"按钮
3. 等待 AI 分析完成
4. 查看推荐的字段映射
5. 验证推荐的合理性
6. 接受或修改推荐
7. 执行任务验证映射正确

**成功条件**:
- AI 映射功能可用
- 推荐结果合理
- 可以接受或修改推荐
- 执行结果正确

**失败条件**:
- AI 映射功能不可用
- 推荐结果不合理
- 无法修改推荐
- 执行结果错误

---

### ETL-016 执行统计

**实现目标**: 统计 ETL 任务的执行情况，包括执行次数、成功率、处理行数等

**测试步骤**:
1. 进入 ETL 任务详情
2. 查看统计面板
3. 验证显示总执行次数
4. 验证显示成功/失败次数
5. 验证显示成功率
6. 验证显示总处理行数
7. 验证显示平均执行时长

**成功条件**:
- 统计数据准确
- 成功率计算正确
- 行数统计正确
- 时长统计正确

**失败条件**:
- 统计数据不准确
- 成功率计算错误
- 行数统计错误
- 时长统计错误

---

## 2.2 实时计算 Flink (FLINK)

**后端代码**: `services/data-api/models/flink.py`
**前端代码**: `web/src/pages/data/streaming/StreamingPage.tsx`

### FLINK-001 Flink 作业创建

**实现目标**: 创建 Flink 实时计算作业，支持 SQL、JAR、Python 类型

**测试步骤**:
1. 进入 Flink 作业管理页面
2. 点击"创建作业"
3. 选择作业类型（SQL/JAR/Python）
4. 填写作业名称和描述
5. 配置作业参数
6. 保存作业

**成功条件**:
- 作业创建成功
- 支持所有作业类型
- 配置正确保存
- 作业在列表显示

**失败条件**:
- 作业创建失败
- 某种类型不支持
- 配置保存失败
- 列表不显示

---

### FLINK-002 Flink 作业列表

**实现目标**: 查看所有 Flink 作业列表，显示状态和运行信息

**测试步骤**:
1. 进入 Flink 作业管理页面
2. 验证列表显示所有作业
3. 验证显示作业名称、类型、状态
4. 验证显示运行时长
5. 测试筛选功能
6. 测试搜索功能

**成功条件**:
- 列表完整显示
- 状态实时更新
- 筛选功能正常
- 搜索结果准确

**失败条件**:
- 列表加载失败
- 状态不更新
- 筛选无效
- 搜索不准确

---

### FLINK-003 Flink 作业启动

**实现目标**: 启动 Flink 作业，提交到集群运行

**测试步骤**:
1. 选择一个已停止的作业
2. 点击"启动"按钮
3. 验证作业提交到集群
4. 验证状态变为 running
5. 验证 Flink UI 可查看作业
6. 验证数据开始处理

**成功条件**:
- 作业启动成功
- 状态正确更新
- Flink UI 可见
- 数据正常处理

**失败条件**:
- 启动失败
- 状态不更新
- Flink UI 不可见
- 数据不处理

---

### FLINK-004 Flink 作业停止

**实现目标**: 停止运行中的 Flink 作业

**测试步骤**:
1. 选择一个运行中的作业
2. 点击"停止"按钮
3. 选择停止方式（取消/保存点停止）
4. 验证作业停止
5. 验证状态变为 stopped
6. 验证保存点创建（如选择）

**成功条件**:
- 作业停止成功
- 状态正确更新
- 保存点创建成功
- 可从保存点恢复

**失败条件**:
- 停止失败
- 状态不更新
- 保存点创建失败
- 无法从保存点恢复

---

### FLINK-005 SQL 作业配置

**实现目标**: 配置 Flink SQL 类型的作业

**测试步骤**:
1. 创建或编辑 SQL 作业
2. 定义 Source 表（如 Kafka Source）
3. 定义 Sink 表（如 MySQL Sink）
4. 编写转换 SQL
5. 验证 SQL 语法
6. 保存配置

**成功条件**:
- Source/Sink 定义正确
- SQL 语法验证通过
- 配置保存成功
- 执行时数据流转正确

**失败条件**:
- Source/Sink 定义失败
- SQL 语法错误
- 配置保存失败
- 数据流转错误

---

### FLINK-006 JAR 作业配置

**实现目标**: 配置 JAR 包类型的 Flink 作业

**测试步骤**:
1. 创建或编辑 JAR 作业
2. 上传 JAR 包
3. 配置入口类
4. 配置程序参数
5. 配置并行度
6. 保存配置

**成功条件**:
- JAR 上传成功
- 入口类识别正确
- 参数传递正确
- 执行成功

**失败条件**:
- JAR 上传失败
- 入口类找不到
- 参数传递错误
- 执行失败

---

### FLINK-007 Python 作业配置

**实现目标**: 配置 Python 类型的 Flink 作业（PyFlink）

**测试步骤**:
1. 创建或编辑 Python 作业
2. 上传 Python 脚本
3. 配置依赖包
4. 配置入口函数
5. 配置资源参数
6. 保存配置

**成功条件**:
- Python 脚本上传成功
- 依赖正确安装
- 执行成功
- 日志正确输出

**失败条件**:
- 脚本上传失败
- 依赖安装失败
- 执行失败
- 日志不输出

---

### FLINK-008 资源配置

**实现目标**: 配置 Flink 作业的 TaskManager 和 JobManager 资源

**测试步骤**:
1. 编辑作业资源配置
2. 配置 JobManager 内存
3. 配置 TaskManager 内存
4. 配置 TaskManager 数量
5. 配置 Slots 数量
6. 保存并启动作业
7. 验证资源分配正确

**成功条件**:
- 资源配置保存成功
- 内存分配正确
- Slots 分配正确
- 作业按配置运行

**失败条件**:
- 资源配置失败
- 内存分配错误
- Slots 不足
- 作业资源不足失败

---

### FLINK-009 Checkpoint 配置

**实现目标**: 配置 Flink 作业的 Checkpoint 和 Savepoint

**测试步骤**:
1. 编辑作业 Checkpoint 配置
2. 启用 Checkpoint
3. 配置 Checkpoint 间隔
4. 配置 Checkpoint 超时
5. 配置 Savepoint 路径
6. 启动作业验证 Checkpoint 生成

**成功条件**:
- Checkpoint 配置成功
- 按间隔生成 Checkpoint
- Savepoint 正确保存
- 可从 Checkpoint/Savepoint 恢复

**失败条件**:
- Checkpoint 配置失败
- Checkpoint 不生成
- Savepoint 保存失败
- 恢复失败

---

### FLINK-010 作业日志查看

**实现目标**: 查看 Flink 作业的执行日志

**测试步骤**:
1. 进入作业详情
2. 切换到日志页签
3. 验证显示 JobManager 日志
4. 验证显示 TaskManager 日志
5. 测试日志实时刷新
6. 测试日志搜索
7. 测试日志下载

**成功条件**:
- 日志正确加载
- JM/TM 日志都可查看
- 实时刷新正常
- 搜索和下载正常

**失败条件**:
- 日志加载失败
- 只能看部分日志
- 不支持实时刷新
- 搜索或下载失败

---

### FLINK-011 运行指标监控

**实现目标**: 监控 Flink 作业的运行指标

**测试步骤**:
1. 启动一个 Flink 作业
2. 进入监控面板
3. 验证显示 records_in
4. 验证显示 records_out
5. 验证显示 bytes_in
6. 验证显示 bytes_out
7. 验证指标实时更新

**成功条件**:
- 指标面板正常显示
- records 计数准确
- bytes 计数准确
- 指标实时更新

**失败条件**:
- 指标面板不显示
- 计数不准确
- 指标不更新
- 指标延迟严重

---

### FLINK-012 SQL 查询保存

**实现目标**: 保存常用的 Flink SQL 查询以便复用

**测试步骤**:
1. 在 SQL 编辑器编写查询
2. 点击"保存查询"
3. 输入查询名称和描述
4. 验证保存成功
5. 在查询列表找到保存的查询
6. 加载保存的查询
7. 测试查询删除

**成功条件**:
- 查询保存成功
- 查询列表可见
- 可加载已保存查询
- 可删除查询

**失败条件**:
- 保存失败
- 列表不显示
- 加载失败
- 删除失败

---

## 2.3 Streaming IDE (SIDE)

**前端代码**: `web/src/pages/data/streaming-ide/StreamingIDEPage.tsx`

### SIDE-001 SQL 编辑器

**实现目标**: 提供在线 Flink SQL 编辑器

**测试步骤**:
1. 进入 Streaming IDE 页面
2. 验证编辑器正常加载
3. 在编辑器中输入 SQL
4. 验证支持多行编辑
5. 测试撤销/重做功能
6. 测试快捷键

**成功条件**:
- 编辑器正常加载
- 支持多行编辑
- 撤销/重做正常
- 快捷键可用

**失败条件**:
- 编辑器加载失败
- 编辑体验差
- 撤销/重做不工作
- 快捷键无效

---

### SIDE-002 语法高亮

**实现目标**: SQL 编辑器支持语法高亮显示

**测试步骤**:
1. 在编辑器中输入 SELECT 语句
2. 验证关键字高亮（SELECT/FROM/WHERE）
3. 验证字符串高亮
4. 验证数字高亮
5. 验证注释高亮
6. 验证函数高亮

**成功条件**:
- 关键字正确高亮
- 字符串正确高亮
- 注释正确高亮
- 颜色区分明显

**失败条件**:
- 无语法高亮
- 高亮不正确
- 颜色难以区分
- 性能问题

---

### SIDE-003 自动补全

**实现目标**: SQL 编辑器支持关键字和表名自动补全

**测试步骤**:
1. 输入 "SEL" 触发补全
2. 验证显示 SELECT 建议
3. 输入表名前缀触发补全
4. 验证显示匹配的表名
5. 输入字段名前缀
6. 验证显示表的字段列表
7. 测试补全性能

**成功条件**:
- 关键字补全正常
- 表名补全正常
- 字段补全正常
- 补全响应快速

**失败条件**:
- 补全不触发
- 建议不准确
- 字段补全不工作
- 补全响应慢

---

### SIDE-004 SQL 执行

**实现目标**: 在线执行 SQL 查询

**测试步骤**:
1. 在编辑器中编写查询 SQL
2. 点击"执行"按钮
3. 验证显示执行中状态
4. 等待执行完成
5. 验证返回执行结果
6. 测试执行失败场景
7. 测试中断执行

**成功条件**:
- SQL 执行成功
- 状态正确显示
- 结果正确返回
- 可中断执行

**失败条件**:
- 执行失败
- 状态不显示
- 结果不返回
- 无法中断

---

### SIDE-005 结果预览

**实现目标**: 预览 SQL 执行结果

**测试步骤**:
1. 执行一个返回数据的 SQL
2. 验证结果表格显示
3. 验证列名正确
4. 验证数据正确
5. 测试大数据量分页
6. 测试结果导出

**成功条件**:
- 结果表格正确显示
- 列名和数据准确
- 分页功能正常
- 支持导出

**失败条件**:
- 表格不显示
- 数据不准确
- 分页不工作
- 不支持导出

---

### SIDE-006 查询历史

**实现目标**: 查看历史执行的 SQL 查询

**测试步骤**:
1. 执行多条 SQL 查询
2. 打开查询历史面板
3. 验证显示历史查询列表
4. 验证显示执行时间和状态
5. 点击历史记录加载到编辑器
6. 测试历史记录搜索

**成功条件**:
- 历史记录完整
- 显示执行时间和状态
- 可加载历史查询
- 搜索功能正常

**失败条件**:
- 历史记录不完整
- 信息显示不全
- 无法加载历史
- 搜索不工作

---

## 2.4 离线处理 (OFF)

**后端代码**: `services/data-api/models/offline.py`
**前端代码**: `web/src/pages/data/offline/OfflinePage.tsx`

### OFF-001 离线任务创建

**实现目标**: 创建离线计算任务

**测试步骤**:
1. 进入离线处理页面
2. 点击"创建任务"
3. 填写任务名称和描述
4. 选择任务类型（Spark/Hive/Presto/Python）
5. 配置任务参数
6. 保存任务

**成功条件**:
- 任务创建成功
- 支持所有类型
- 配置正确保存
- 任务列表可见

**失败条件**:
- 创建失败
- 类型不支持
- 配置丢失
- 列表不显示

---

### OFF-002 离线任务列表

**实现目标**: 查看离线任务列表

**测试步骤**:
1. 进入离线处理页面
2. 验证列表显示所有任务
3. 验证显示任务名称、类型、状态
4. 验证显示最后执行时间
5. 测试筛选功能
6. 测试搜索功能

**成功条件**:
- 列表完整显示
- 信息准确
- 筛选正常
- 搜索准确

**失败条件**:
- 列表不完整
- 信息错误
- 筛选无效
- 搜索不准

---

### OFF-003 离线任务编辑

**实现目标**: 修改离线任务配置

**测试步骤**:
1. 选择任务点击编辑
2. 验证原配置加载
3. 修改任务参数
4. 修改资源配置
5. 保存修改

**成功条件**:
- 配置正确加载
- 修改保存成功
- 修改生效
- 版本更新

**失败条件**:
- 配置加载失败
- 保存失败
- 修改不生效
- 版本混乱

---

### OFF-004 离线任务删除

**实现目标**: 删除离线任务

**测试步骤**:
1. 选择未运行的任务
2. 点击删除
3. 确认删除
4. 验证任务移除
5. 尝试删除运行中任务

**成功条件**:
- 删除成功
- 列表更新
- 运行中提示停止
- 记录日志

**失败条件**:
- 删除失败
- 列表不更新
- 运行中被删除
- 无确认

---

### OFF-005 离线任务执行

**实现目标**: 手动触发离线任务执行

**测试步骤**:
1. 选择任务点击执行
2. 验证状态变为 running
3. 等待执行完成
4. 验证输出数据正确
5. 验证日志生成

**成功条件**:
- 执行成功
- 状态更新
- 数据正确
- 日志完整

**失败条件**:
- 执行失败
- 状态不变
- 数据错误
- 无日志

---

### OFF-006 Spark SQL 配置

**实现目标**: 配置 Spark SQL 类型的离线任务

**测试步骤**:
1. 创建 Spark SQL 任务
2. 编写 Spark SQL
3. 配置数据源
4. 配置输出表
5. 执行验证

**成功条件**:
- SQL 配置成功
- 数据源连接正常
- 输出正确
- 执行成功

**失败条件**:
- 配置失败
- 数据源连接失败
- 输出错误
- 执行失败

---

### OFF-007 Hive SQL 配置

**实现目标**: 配置 Hive SQL 类型的离线任务

**测试步骤**:
1. 创建 Hive SQL 任务
2. 连接 Hive 数据源
3. 编写 Hive SQL
4. 配置输出
5. 执行验证

**成功条件**:
- Hive 连接成功
- SQL 执行成功
- 输出正确
- 性能可接受

**失败条件**:
- 连接失败
- SQL 执行失败
- 输出错误
- 性能太差

---

### OFF-008 Presto 配置

**实现目标**: 配置 Presto 类型的离线任务

**测试步骤**:
1. 创建 Presto 任务
2. 连接 Presto 集群
3. 编写 Presto SQL
4. 配置 Catalog
5. 执行验证

**成功条件**:
- Presto 连接成功
- Catalog 配置正确
- SQL 执行成功
- 查询性能好

**失败条件**:
- 连接失败
- Catalog 配置错误
- 执行失败
- 性能差

---

### OFF-009 Python 脚本配置

**实现目标**: 配置 Python 脚本类型的离线任务

**测试步骤**:
1. 创建 Python 任务
2. 上传或编写 Python 脚本
3. 配置依赖包
4. 配置输入输出
5. 执行验证

**成功条件**:
- 脚本配置成功
- 依赖安装成功
- 执行成功
- 输出正确

**失败条件**:
- 配置失败
- 依赖安装失败
- 执行失败
- 输出错误

---

### OFF-010 资源配置

**实现目标**: 配置离线任务的计算资源

**测试步骤**:
1. 编辑任务资源配置
2. 配置 Executor 内存
3. 配置 Executor 核心数
4. 配置 Executor 数量
5. 启动任务验证资源

**成功条件**:
- 资源配置成功
- 内存分配正确
- 核心分配正确
- 实例数正确

**失败条件**:
- 配置失败
- 内存不足
- 核心不足
- 实例数错误

---

### OFF-011 调度配置

**实现目标**: 配置离线任务的 cron 定时调度

**测试步骤**:
1. 编辑任务调度配置
2. 配置 cron 表达式
3. 启用调度
4. 验证调度注册
5. 验证按时触发

**成功条件**:
- 调度配置成功
- cron 解析正确
- 任务按时触发
- 状态显示正确

**失败条件**:
- 配置失败
- cron 解析错误
- 不按时触发
- 状态不更新

---

### OFF-012 依赖配置

**实现目标**: 配置离线任务之间的依赖关系

**测试步骤**:
1. 创建多个任务
2. 配置任务 B 依赖任务 A
3. 触发调度
4. 验证 A 先执行
5. 验证 A 完成后 B 执行
6. 验证 A 失败时 B 不执行

**成功条件**:
- 依赖配置成功
- 执行顺序正确
- A 失败时 B 跳过
- 状态传递正确

**失败条件**:
- 依赖配置失败
- 顺序不正确
- A 失败 B 仍执行
- 状态传递错误

---

### OFF-013 输出配置

**实现目标**: 配置离线任务的输出目标

**测试步骤**:
1. 编辑任务输出配置
2. 配置输出表名
3. 配置输出路径（HDFS/S3）
4. 配置输出格式（Parquet/ORC/CSV）
5. 执行任务验证输出

**成功条件**:
- 输出配置成功
- 表创建/写入正确
- 路径写入正确
- 格式正确

**失败条件**:
- 配置失败
- 表写入失败
- 路径写入失败
- 格式错误

---

### OFF-014 执行日志查看

**实现目标**: 查看离线任务的执行日志

**测试步骤**:
1. 进入任务详情
2. 查看执行历史
3. 点击某次执行查看日志
4. 验证日志完整
5. 测试日志搜索
6. 测试日志下载

**成功条件**:
- 日志完整显示
- 支持搜索
- 支持下载
- 错误信息清晰

**失败条件**:
- 日志不完整
- 搜索不工作
- 下载失败
- 错误不清晰

---

## 2.5 Kafka 流处理 (KFK)

**后端代码**: `services/data-api/services/kafka_stream_service.py`
**前端代码**: `web/src/pages/data/streaming/KafkaStreamingPage.tsx`

### KFK-001 Kafka 消费配置

**实现目标**: 配置 Kafka 消费者

**测试步骤**:
1. 进入 Kafka 管理页面
2. 创建消费者配置
3. 配置 Bootstrap Servers
4. 配置 Consumer Group
5. 配置 Topic
6. 配置反序列化方式
7. 测试消费

**成功条件**:
- 配置保存成功
- 连接 Kafka 成功
- 消费数据正常
- 反序列化正确

**失败条件**:
- 配置失败
- 连接失败
- 消费失败
- 反序列化错误

---

### KFK-002 Kafka 生产配置

**实现目标**: 配置 Kafka 生产者

**测试步骤**:
1. 创建生产者配置
2. 配置 Bootstrap Servers
3. 配置 Topic
4. 配置序列化方式
5. 配置 Acks 策略
6. 测试发送消息

**成功条件**:
- 配置保存成功
- 连接成功
- 消息发送成功
- 序列化正确

**失败条件**:
- 配置失败
- 连接失败
- 发送失败
- 序列化错误

---

### KFK-003 Topic 管理

**实现目标**: 管理 Kafka Topic

**测试步骤**:
1. 进入 Topic 管理页面
2. 查看 Topic 列表
3. 创建新 Topic
4. 配置分区数和副本数
5. 修改 Topic 配置
6. 删除 Topic

**成功条件**:
- Topic 列表正确
- 创建成功
- 配置正确
- 删除成功

**失败条件**:
- 列表加载失败
- 创建失败
- 配置错误
- 删除失败

---

### KFK-004 消息预览

**实现目标**: 预览 Topic 中的消息

**测试步骤**:
1. 选择一个 Topic
2. 点击"预览消息"
3. 验证显示最新消息
4. 验证消息内容正确
5. 测试按 offset 查看
6. 测试按时间范围查看

**成功条件**:
- 消息正确显示
- 内容可读
- offset 查看正常
- 时间范围查看正常

**失败条件**:
- 消息不显示
- 内容乱码
- offset 查看失败
- 时间范围无效

---

## 统计汇总

| 模块 | 功能数 | 功能编号范围 |
|------|--------|-------------|
| ETL 批量处理 (ETL) | 16 | ETL-001 ~ ETL-016 |
| 实时计算 Flink (FLINK) | 12 | FLINK-001 ~ FLINK-012 |
| Streaming IDE (SIDE) | 6 | SIDE-001 ~ SIDE-006 |
| 离线处理 (OFF) | 14 | OFF-001 ~ OFF-014 |
| Kafka 流处理 (KFK) | 4 | KFK-001 ~ KFK-004 |
| **总计** | **52** | |

---

## 版本历史

| 版本 | 日期 | 变更说明 |
|------|------|---------|
| 1.0.0 | 2026-02-09 | 初始版本 |
