# Prometheus Alert Rules for ONE-DATA-STUDIO
# Sprint 31: Production Readiness Enhancement
#
# Alert severity levels:
# - critical: Immediate action required (pages on-call)
# - warning: Needs attention soon (creates ticket)
# - info: Informational (logged only)

groups:
  # API Performance Alerts
  - name: api-performance
    interval: 30s
    rules:
      - alert: APIHighLatency
        expr: |
          histogram_quantile(0.99,
            sum(rate(http_request_duration_seconds_bucket{job="agent-api"}[5m])) by (le, endpoint)
          ) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "API P99 latency is high"
          description: "P99 latency for endpoint {{ $labels.endpoint }} is {{ $value | humanizeDuration }} (threshold: 1s)"
          runbook_url: "https://wiki.example.com/runbooks/api-latency"

      - alert: APICriticalLatency
        expr: |
          histogram_quantile(0.99,
            sum(rate(http_request_duration_seconds_bucket{job="agent-api"}[5m])) by (le, endpoint)
          ) > 5
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "API P99 latency is critically high"
          description: "P99 latency for endpoint {{ $labels.endpoint }} is {{ $value | humanizeDuration }} (threshold: 5s)"

      - alert: APIHighErrorRate
        expr: |
          sum(rate(http_requests_total{job="agent-api", status=~"5.."}[5m])) by (service)
          /
          sum(rate(http_requests_total{job="agent-api"}[5m])) by (service)
          > 0.05
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "API error rate exceeds 5%"
          description: "Service {{ $labels.service }} has error rate of {{ $value | humanizePercentage }}"

      - alert: APICriticalErrorRate
        expr: |
          sum(rate(http_requests_total{job="agent-api", status=~"5.."}[5m])) by (service)
          /
          sum(rate(http_requests_total{job="agent-api"}[5m])) by (service)
          > 0.10
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "API error rate exceeds 10%"
          description: "Service {{ $labels.service }} has critical error rate of {{ $value | humanizePercentage }}"

  # Pod Health Alerts
  - name: pod-health
    interval: 30s
    rules:
      - alert: PodHighRestartRate
        expr: |
          increase(kube_pod_container_status_restarts_total{namespace=~"one-data.*"}[1h]) > 3
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Pod is restarting frequently"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted {{ $value | humanize }} times in the last hour"

      - alert: PodCrashLooping
        expr: |
          increase(kube_pod_container_status_restarts_total{namespace=~"one-data.*"}[1h]) > 5
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping with {{ $value | humanize }} restarts in the last hour"

      - alert: PodNotReady
        expr: |
          kube_pod_status_ready{namespace=~"one-data.*", condition="true"} == 0
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Pod is not ready"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been not ready for more than 10 minutes"

      - alert: PodMemoryHigh
        expr: |
          container_memory_usage_bytes{namespace=~"one-data.*"}
          /
          container_spec_memory_limit_bytes{namespace=~"one-data.*"}
          > 0.9
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Pod memory usage is high"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} memory usage is at {{ $value | humanizePercentage }}"

      - alert: PodCPUHigh
        expr: |
          rate(container_cpu_usage_seconds_total{namespace=~"one-data.*"}[5m])
          /
          container_spec_cpu_quota{namespace=~"one-data.*"} * 100000
          > 0.9
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Pod CPU usage is high"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} CPU usage is at {{ $value | humanizePercentage }}"

  # Database Alerts
  - name: database
    interval: 30s
    rules:
      - alert: MySQLReplicationLag
        expr: |
          mysql_slave_status_seconds_behind_master > 30
        for: 5m
        labels:
          severity: warning
          team: database
        annotations:
          summary: "MySQL replication is lagging"
          description: "MySQL replica is {{ $value | humanizeDuration }} behind master"

      - alert: MySQLReplicationCriticalLag
        expr: |
          mysql_slave_status_seconds_behind_master > 120
        for: 2m
        labels:
          severity: critical
          team: database
        annotations:
          summary: "MySQL replication lag is critical"
          description: "MySQL replica is {{ $value | humanizeDuration }} behind master"

      - alert: MySQLConnectionsHigh
        expr: |
          mysql_global_status_threads_connected
          /
          mysql_global_variables_max_connections
          > 0.8
        for: 5m
        labels:
          severity: warning
          team: database
        annotations:
          summary: "MySQL connections are high"
          description: "MySQL is using {{ $value | humanizePercentage }} of max connections"

      - alert: MySQLDown
        expr: up{job="mysql"} == 0
        for: 1m
        labels:
          severity: critical
          team: database
        annotations:
          summary: "MySQL is down"
          description: "MySQL instance {{ $labels.instance }} is not responding"

  # Storage Alerts
  - name: storage
    interval: 1m
    rules:
      - alert: DiskSpaceWarning
        expr: |
          (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.2
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Disk space is running low"
          description: "Node {{ $labels.instance }} has only {{ $value | humanizePercentage }} disk space remaining"

      - alert: DiskSpaceCritical
        expr: |
          (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.1
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Disk space is critically low"
          description: "Node {{ $labels.instance }} has only {{ $value | humanizePercentage }} disk space remaining"

      - alert: PVCUsageHigh
        expr: |
          kubelet_volume_stats_used_bytes{namespace=~"one-data.*"}
          /
          kubelet_volume_stats_capacity_bytes{namespace=~"one-data.*"}
          > 0.8
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "PVC usage is high"
          description: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is at {{ $value | humanizePercentage }} capacity"

  # Circuit Breaker Alerts
  - name: circuit-breaker
    interval: 30s
    rules:
      - alert: CircuitBreakerOpen
        expr: |
          circuit_breaker_state{state="open"} == 1
        for: 1m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Circuit breaker is open"
          description: "Circuit breaker {{ $labels.name }} is in open state, blocking requests to dependent service"

      - alert: CircuitBreakerHighFailureRate
        expr: |
          circuit_breaker_failure_rate > 0.5
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Circuit breaker has high failure rate"
          description: "Circuit breaker {{ $labels.name }} has failure rate of {{ $value | humanizePercentage }}"

  # Certificate Alerts
  - name: certificates
    interval: 1h
    rules:
      - alert: CertificateExpiringSoon
        expr: |
          (x509_cert_expiry - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Certificate expiring soon"
          description: "Certificate {{ $labels.cn }} expires in {{ $value | humanize }} days"

      - alert: CertificateExpiringCritical
        expr: |
          (x509_cert_expiry - time()) / 86400 < 7
        for: 1h
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Certificate expiring in less than 7 days"
          description: "Certificate {{ $labels.cn }} expires in {{ $value | humanize }} days"

  # LLM Service Alerts
  - name: llm-service
    interval: 30s
    rules:
      - alert: LLMServiceDown
        expr: up{job="vllm-serving"} == 0
        for: 2m
        labels:
          severity: critical
          team: ml
        annotations:
          summary: "LLM service is down"
          description: "vLLM serving instance {{ $labels.instance }} is not responding"

      - alert: LLMHighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(llm_request_duration_seconds_bucket[5m])) by (le, model)
          ) > 30
        for: 5m
        labels:
          severity: warning
          team: ml
        annotations:
          summary: "LLM inference latency is high"
          description: "Model {{ $labels.model }} P95 latency is {{ $value | humanizeDuration }}"

      - alert: LLMGPUMemoryHigh
        expr: |
          nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes > 0.95
        for: 5m
        labels:
          severity: warning
          team: ml
        annotations:
          summary: "GPU memory usage is high"
          description: "GPU {{ $labels.gpu }} memory usage is at {{ $value | humanizePercentage }}"

  # Vector Database Alerts
  - name: milvus
    interval: 30s
    rules:
      - alert: MilvusDown
        expr: up{job="milvus"} == 0
        for: 2m
        labels:
          severity: critical
          team: database
        annotations:
          summary: "Milvus is down"
          description: "Milvus instance {{ $labels.instance }} is not responding"

      - alert: MilvusSearchLatencyHigh
        expr: |
          histogram_quantile(0.95,
            sum(rate(milvus_query_node_search_latency_bucket[5m])) by (le)
          ) > 1
        for: 5m
        labels:
          severity: warning
          team: database
        annotations:
          summary: "Milvus search latency is high"
          description: "Milvus P95 search latency is {{ $value | humanizeDuration }}"
