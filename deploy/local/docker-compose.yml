# ONE-DATA-STUDIO Local Development Environment
#
# Usage:
#   docker-compose up -d              # Start all services
#   docker-compose up -d mysql redis  # Start specific services
#   docker-compose logs -f            # View logs
#   docker-compose down               # Stop all services

version: '3.8'

services:
  # ============================================
  # Databases
  # ============================================
  mysql:
    image: mysql:8.0
    container_name: one-data-mysql
    environment:
      # SECURITY: All passwords MUST be set via environment variables
      # DO NOT use default values in production
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:?MYSQL_ROOT_PASSWORD must be set}
      MYSQL_DATABASE: ${MYSQL_DATABASE:-onedata}
      MYSQL_USER: ${MYSQL_USER:-onedata}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - one-data-network

  redis:
    image: redis:7-alpine
    container_name: one-data-redis
    # SECURITY: Redis password authentication enabled
    command: redis-server --requirepass ${REDIS_PASSWORD:?REDIS_PASSWORD must be set}
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - one-data-network

  minio:
    image: minio/minio:latest  # Using latest for development
    container_name: one-data-minio
    environment:
      # SECURITY: MinIO credentials MUST be set via environment variables
      # DO NOT use default values in production
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:?MINIO_ROOT_USER must be set}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:?MINIO_ROOT_PASSWORD must be set}
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - one-data-network

  milvus:
    image: milvusdb/milvus:v2.3.0
    container_name: one-data-milvus
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      MINIO_ACCESS_KEY_ID: ${MINIO_ROOT_USER:?MINIO_ROOT_USER must be set}
      MINIO_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:?MINIO_ROOT_PASSWORD must be set}
    ports:
      - "19530:19530"
      - "9091:9091"
    volumes:
      - milvus_data:/var/lib/milvus
    depends_on:
      - etcd
      - minio
    networks:
      - one-data-network

  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    container_name: one-data-etcd
    environment:
      ETCD_AUTO_COMPACTION_MODE: revision
      ETCD_AUTO_COMPACTION_RETENTION: "1000"
      ETCD_QUOTA_BACKEND_BYTES: "4294967296"
      ETCD_SNAPSHOT_COUNT: "50000"
    command: etcd --advertise-client-urls=http://0.0.0.0:2379 --listen-client-urls=http://0.0.0.0:2379
    volumes:
      - etcd_data:/etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - one-data-network

  # ============================================
  # Metadata Governance (OpenMetadata)
  # ============================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.2
    container_name: one-data-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q 'green\\|yellow'"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - one-data-network

  openmetadata:
    image: openmetadata/server:1.4.0
    container_name: one-data-openmetadata
    environment:
      # Database configuration (reuse existing MySQL)
      DB_DRIVER_CLASS: com.mysql.cj.jdbc.Driver
      DB_SCHEME: mysql
      DB_HOST: mysql
      DB_PORT: 3306
      DB_USER: ${MYSQL_USER:-onedata}
      DB_USER_PASSWORD: ${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}
      OM_DATABASE: openmetadata_db
      # Elasticsearch configuration
      ELASTICSEARCH_HOST: elasticsearch
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_SCHEME: http
      # Server configuration
      SERVER_HOST: openmetadata
      SERVER_PORT: 8585
      # Authentication (basic auth for development)
      AUTHENTICATION_PROVIDER: basic
      AUTHENTICATION_PUBLIC_KEYS: "[http://localhost:8585/api/v1/system/config/jwks]"
      AUTHENTICATION_AUTHORITY: "http://localhost:8585"
      AUTHENTICATION_CLIENT_ID: "open-metadata"
      AUTHORIZER_CLASS_NAME: "org.openmetadata.service.security.DefaultAuthorizer"
      AUTHORIZER_REQUEST_FILTER: "org.openmetadata.service.security.JwtFilter"
      AUTHORIZER_ADMIN_PRINCIPALS: "[admin]"
      AUTHORIZER_PRINCIPAL_DOMAIN: "open-metadata.org"
      # Airflow disabled (using external orchestration)
      PIPELINE_SERVICE_CLIENT_ENABLED: false
    ports:
      - "8585:8585"
    depends_on:
      mysql:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8585/api/v1/system/version | grep -q 'version'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - one-data-network

  # ============================================
  # ETL Engine (Pentaho Kettle / Webspoon)
  # ============================================
  # Webspoon provides web-based Pentaho Data Integration (Kettle)
  # with Carte server for remote transformation execution
  kettle:
    image: hiromuhota/webspoon:0.9.0.22
    container_name: one-data-kettle
    environment:
      # Carte server configuration for remote execution
      KETTLE_CARTE_ENABLED: "true"
      KETTLE_CARTE_PORT: 8181
      # SECURITY: Carte credentials MUST be set via environment variables
      KETTLE_CARTE_USER: ${KETTLE_CARTE_USER:-cluster}
      KETTLE_CARTE_PASSWORD: ${KETTLE_CARTE_PASSWORD:?KETTLE_CARTE_PASSWORD must be set}
      # Java memory settings
      JAVA_OPTS: "-Xms512m -Xmx2g"
      # Database JDBC drivers will be auto-loaded from /jdbc-drivers volume
    ports:
      - "8089:8080"   # Webspoon UI
      - "8181:8181"   # Carte API
    volumes:
      - kettle_data:/home/tomcat/.kettle
      - kettle_jdbc:/jdbc-drivers
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8080/spoon/spoon || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - one-data-network

  # ============================================
  # Apache Hop ETL Engine (Kettle alternative)
  # ============================================
  hop-server:
    image: apache/hop:2.8.0
    container_name: one-data-hop-server
    command: ["hop-server", "/config/hop-server.xml"]
    environment:
      HOP_SERVER_USER: ${HOP_SERVER_USER:-cluster}
      HOP_SERVER_PASSWORD: ${HOP_SERVER_PASSWORD:-cluster}
      HOP_SERVER_PORT: "8182"
      HOP_LOG_LEVEL: Basic
    ports:
      - "8182:8182"
    volumes:
      - hop_data:/home/hop
      - hop_config:/config
    networks:
      - one-data-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8182/hop/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    profiles:
      - etl

  # ============================================
  # ShardingSphere Proxy (Transparent Data Masking)
  # ============================================
  shardingsphere-proxy:
    image: apache/shardingsphere-proxy:5.4.1
    container_name: one-data-shardingsphere-proxy
    environment:
      PROXY_PORT: "3307"
    ports:
      - "3307:3307"     # MySQL protocol port
      - "33071:33071"   # Admin HTTP API
    volumes:
      - shardingsphere_conf:/opt/shardingsphere-proxy/conf
      - shardingsphere_ext:/opt/shardingsphere-proxy/ext-lib
    networks:
      - one-data-network
    depends_on:
      mysql:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping -h localhost -P 3307 -u root || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    profiles:
      - security

  # ============================================
  # Authentication
  # ============================================
  # Keycloak 初始化服务 - 创建 realm 和客户端
  keycloak-init:
    image: curlimages/curl:latest
    container_name: one-data-keycloak-init
    volumes:
      - ./config/keycloak/setup-realm.sh:/setup-realm.sh:ro
    command: ["/bin/sh", "/setup-realm.sh"]
    environment:
      KEYCLOAK_URL: http://keycloak:8080
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD:-admin}
    depends_on:
      keycloak:
        condition: service_healthy
    networks:
      - one-data-network
    restart: "no"

  keycloak:
    image: quay.io/keycloak/keycloak:23.0
    container_name: one-data-keycloak
    environment:
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD:-admin}
      KC_DB: mysql
      KC_DB_URL: jdbc:mysql://mysql:3306/${MYSQL_DATABASE:-onedata}
      KC_DB_USERNAME: ${MYSQL_USER:-onedata}
      KC_DB_PASSWORD: ${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}
      # hostname 配置
      KC_HOSTNAME_STRICT: "false"
      KC_HOSTNAME_PORT: "8080"
      KC_HTTP_ENABLED: "true"
    command: start-dev
    ports:
      - "8080:8080"
    depends_on:
      mysql:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "cat /proc/net/tcp | grep ':1F90 ' || true"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - one-data-network

  # ============================================
  # AI/LLM Services (vLLM)
  # ============================================
  # vLLM Chat Service - for LLM inference (Text generation, Text-to-SQL, etc.)
  # Requires NVIDIA GPU with CUDA support
  # To run without GPU, set VLLM_MOCK_ENABLED=true
  vllm-chat:
    image: vllm/vllm-openai:latest
    container_name: one-data-vllm-chat
    # GPU configuration - uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    environment:
      # Model configuration
      # For development/testing without GPU, use a smaller model or mock mode
      VLLM_MODEL: ${VLLM_CHAT_MODEL:-Qwen/Qwen2.5-1.5B-Instruct}
      # Server configuration
      VLLM_HOST: 0.0.0.0
      VLLM_PORT: 8000
      # GPU memory utilization (0.0-1.0)
      VLLM_GPU_MEMORY_UTILIZATION: ${VLLM_GPU_MEMORY_UTILIZATION:-0.8}
      # Trust remote code for custom models
      VLLM_TRUST_REMOTE_CODE: "true"
      # HuggingFace token for gated models
      HF_TOKEN: ${HF_TOKEN:-}
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model ${VLLM_CHAT_MODEL:-Qwen/Qwen2.5-1.5B-Instruct}
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 4096
    ports:
      - "8010:8000"
    volumes:
      - vllm_cache:/root/.cache/huggingface
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s  # Model loading takes time
    networks:
      - one-data-network
    profiles:
      - ai  # Only start with: docker-compose --profile ai up

  # vLLM Embedding Service - for text vectorization
  vllm-embed:
    image: vllm/vllm-openai:latest
    container_name: one-data-vllm-embed
    # GPU configuration - uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    environment:
      VLLM_MODEL: ${VLLM_EMBED_MODEL:-BAAI/bge-base-zh-v1.5}
      VLLM_HOST: 0.0.0.0
      VLLM_PORT: 8000
      VLLM_GPU_MEMORY_UTILIZATION: ${VLLM_GPU_MEMORY_UTILIZATION:-0.3}
      VLLM_TRUST_REMOTE_CODE: "true"
      HF_TOKEN: ${HF_TOKEN:-}
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model ${VLLM_EMBED_MODEL:-BAAI/bge-base-zh-v1.5}
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 512
    ports:
      - "8011:8000"
    volumes:
      - vllm_cache:/root/.cache/huggingface
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s
    networks:
      - one-data-network
    profiles:
      - ai  # Only start with: docker-compose --profile ai up

  # Ollama - Local LLM service (alternative to vLLM, no GPU required)
  ollama:
    image: ollama/ollama:latest
    container_name: one-data-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - one-data-network
    profiles:
      - ai  # Only start with: docker-compose --profile ai up

  # ============================================
  # Data Labeling (Label Studio)
  # ============================================
  label-studio-postgresql:
    image: postgres:15.4-alpine
    container_name: one-data-label-studio-pg
    environment:
      POSTGRES_USER: ${LABEL_STUDIO_DB_USER:-labelstudio}
      POSTGRES_PASSWORD: ${LABEL_STUDIO_DB_PASSWORD:-labelstudio}
      POSTGRES_DB: labelstudio
    ports:
      - "5434:5432"
    volumes:
      - label_studio_pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${LABEL_STUDIO_DB_USER:-labelstudio}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - one-data-network

  label-studio:
    image: heartexlabs/label-studio:1.11.0
    container_name: one-data-label-studio
    environment:
      DJANGO_DB: default
      POSTGRE_NAME: labelstudio
      POSTGRE_USER: ${LABEL_STUDIO_DB_USER:-labelstudio}
      POSTGRE_PASSWORD: ${LABEL_STUDIO_DB_PASSWORD:-labelstudio}
      POSTGRE_HOST: label-studio-postgresql
      POSTGRE_PORT: 5432
      LABEL_STUDIO_HOST: ${LABEL_STUDIO_HOST:-http://localhost:8009}
      LABEL_STUDIO_USER_TOKEN: ${LABEL_STUDIO_API_TOKEN:-}
    ports:
      - "8009:8080"
    volumes:
      - label_studio_data:/label-studio/data
    depends_on:
      label-studio-postgresql:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - one-data-network

  # ============================================
  # Application Services
  # ============================================
  agent-api:
    build:
      context: ../../
      dockerfile: deploy/dockerfiles/agent-api/Dockerfile
    container_name: one-data-agent-api
    environment:
      # SECURITY: All credentials MUST be set via environment variables - no default fallbacks
      DATABASE_URL: mysql+pymysql://${MYSQL_USER:-onedata}:${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}@mysql:3306/${MYSQL_DATABASE:-onedata}
      REDIS_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/0
      MILVUS_HOST: milvus
      MILVUS_PORT: 19530
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:?MINIO_ROOT_USER must be set}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:?MINIO_ROOT_PASSWORD must be set}
      # LLM API configuration (through openai-proxy or direct vLLM)
      MODEL_API_URL: ${MODEL_API_URL:-http://openai-proxy:8000}
      # Direct vLLM endpoints for high-performance scenarios
      VLLM_CHAT_URL: ${VLLM_CHAT_URL:-http://vllm-chat:8000}
      VLLM_EMBED_URL: ${VLLM_EMBED_URL:-http://vllm-embed:8000}
      # Model configuration
      CHAT_MODEL: ${VLLM_CHAT_MODEL:-Qwen/Qwen2.5-1.5B-Instruct}
      EMBEDDING_MODEL: ${VLLM_EMBED_MODEL:-BAAI/bge-base-zh-v1.5}
      EMBEDDING_DIM: ${EMBEDDING_DIM:-768}
      # Vector service configuration
      VECTOR_ENABLED: ${VECTOR_ENABLED:-true}
      VECTOR_FALLBACK_ENABLED: ${VECTOR_FALLBACK_ENABLED:-true}
      EMBEDDING_MOCK_ENABLED: ${EMBEDDING_MOCK_ENABLED:-false}
      PORT: 8000
      # Development mode: disable authentication
      AUTH_MODE: "false"
    ports:
      - "8000:8000"
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
      milvus:
        condition: service_started
    networks:
      - one-data-network

  data-api:
    build:
      context: ../../
      dockerfile: deploy/dockerfiles/data-api/Dockerfile
    container_name: one-data-data-api
    volumes:
      - ge_data:/data/ge
    environment:
      # SECURITY: All credentials MUST be set via environment variables - no default fallbacks
      MYSQL_HOST: mysql
      MYSQL_PORT: 3306
      MYSQL_USER: ${MYSQL_USER:-onedata}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}
      MYSQL_DATABASE: ${MYSQL_DATABASE:-onedata}
      REDIS_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/0
      PORT: 8001
      # Development mode: disable authentication
      AUTH_MODE: "false"
      # OpenMetadata integration
      OPENMETADATA_HOST: openmetadata
      OPENMETADATA_PORT: 8585
      OPENMETADATA_ENABLED: ${OPENMETADATA_ENABLED:-true}
      # vLLM service for AI features (metadata annotation, sensitivity scan, ETL recommendations)
      VLLM_CHAT_URL: ${VLLM_CHAT_URL:-http://vllm-chat:8000}
      VLLM_CHAT_MODEL: ${VLLM_CHAT_MODEL:-Qwen/Qwen2.5-1.5B-Instruct}
      AI_FEATURES_ENABLED: ${AI_FEATURES_ENABLED:-true}
      # Kettle/PDI ETL engine integration
      KETTLE_CARTE_URL: ${KETTLE_CARTE_URL:-http://kettle:8181}
      KETTLE_CARTE_USER: ${KETTLE_CARTE_USER:-cluster}
      KETTLE_CARTE_PASSWORD: ${KETTLE_CARTE_PASSWORD:?KETTLE_CARTE_PASSWORD must be set}
      KETTLE_ENABLED: ${KETTLE_ENABLED:-true}
      # Great Expectations data quality engine
      GE_ENABLED: ${GE_ENABLED:-false}
      GE_CONTEXT_ROOT_DIR: /data/ge
      # Apache Hop ETL engine integration
      HOP_SERVER_URL: ${HOP_SERVER_URL:-http://hop-server:8182}
      HOP_SERVER_USER: ${HOP_SERVER_USER:-cluster}
      HOP_SERVER_PASSWORD: ${HOP_SERVER_PASSWORD:-cluster}
      HOP_ENABLED: ${HOP_ENABLED:-false}
      # Default ETL engine: auto, kettle, hop
      ETL_ENGINE: ${ETL_ENGINE:-auto}
      # ShardingSphere Proxy integration (transparent masking)
      SHARDINGSPHERE_PROXY_URL: ${SHARDINGSPHERE_PROXY_URL:-shardingsphere-proxy:3307}
      SHARDINGSPHERE_ADMIN_URL: ${SHARDINGSPHERE_ADMIN_URL:-http://shardingsphere-proxy:33071}
      SHARDINGSPHERE_USER: ${SHARDINGSPHERE_USER:-root}
      SHARDINGSPHERE_PASSWORD: ${SHARDINGSPHERE_PASSWORD:-root}
      SHARDINGSPHERE_ENABLED: ${SHARDINGSPHERE_ENABLED:-false}
    ports:
      - "8001:8001"
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - one-data-network

  openai-proxy:
    build:
      context: ../../
      dockerfile: deploy/dockerfiles/openai-proxy/Dockerfile
    container_name: one-data-openai-proxy
    environment:
      # SECURITY: All credentials MUST be set via environment variables - no default fallbacks
      REDIS_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/0
      # Development mode: disable authentication
      AUTH_MODE: "false"
      # vLLM service endpoints for model inference
      VLLM_CHAT_URL: ${VLLM_CHAT_URL:-http://vllm-chat:8000}
      VLLM_EMBED_URL: ${VLLM_EMBED_URL:-http://vllm-embed:8000}
      # Ollama service endpoint
      OLLAMA_URL: ${OLLAMA_URL:-http://ollama:11434}
      # LLM backend priority: auto, vllm, ollama, openai
      LLM_BACKEND: ${LLM_BACKEND:-auto}
      # Default model names (must match vLLM deployed models)
      DEFAULT_CHAT_MODEL: ${VLLM_CHAT_MODEL:-Qwen/Qwen2.5-1.5B-Instruct}
      DEFAULT_EMBED_MODEL: ${VLLM_EMBED_MODEL:-BAAI/bge-base-zh-v1.5}
    ports:
      - "8003:8000"
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - one-data-network

  admin-api:
    build:
      context: ../../
      dockerfile: deploy/dockerfiles/admin-api/Dockerfile
    container_name: one-data-admin-api
    volumes:
      # 开发模式：挂载本地源代码，修改后需重启容器
      - ../../services/admin-api:/app
    environment:
      # SECURITY: All credentials MUST be set via environment variables - no default fallbacks
      DATABASE_URL: mysql+pymysql://${MYSQL_USER:-onedata}:${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}@mysql:3306/${MYSQL_DATABASE:-onedata}
      REDIS_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/0
      PORT: 8004
      # Development mode: disable authentication
      AUTH_MODE: "false"
      STRICT_AUTH_MODE: "false"
    ports:
      - "8004:8004"
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - one-data-network

  model-api:
    build:
      context: ../../
      dockerfile: deploy/dockerfiles/model-api/Dockerfile
    container_name: one-data-model-api
    volumes:
      # 开发模式：挂载本地源代码，修改后需重启容器
      - ../../services/model-api:/app
    environment:
      # SECURITY: All credentials MUST be set via environment variables - no default fallbacks
      MODEL_DATABASE_URL: mysql+pymysql://${MYSQL_USER:-onedata}:${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}@mysql:3306/${MYSQL_DATABASE:-onedata}
      REDIS_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/0
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:?MINIO_ROOT_USER must be set}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:?MINIO_ROOT_PASSWORD must be set}
      HF_TOKEN: ${HF_TOKEN:-}
      # Development mode: disable authentication
      AUTH_MODE: "false"
      # Label Studio integration
      LABEL_STUDIO_URL: ${LABEL_STUDIO_URL:-http://label-studio:8080}
      LABEL_STUDIO_API_TOKEN: ${LABEL_STUDIO_API_TOKEN:-}
    ports:
      - "8002:8002"
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - one-data-network

  # ============================================
  # OCR Services
  # ============================================
  ocr-service:
    build:
      context: ../../services/ocr-service
      dockerfile: Dockerfile
    container_name: one-data-ocr-service
    environment:
      # Database configuration
      DATABASE_URL: mysql+pymysql://${MYSQL_USER:-onedata}:${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}@mysql:3306/${MYSQL_DATABASE:-onedata}
      # Redis configuration
      REDIS_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/0
      # OpenAI API for AI extraction
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_API_BASE: ${OPENAI_API_BASE:-http://openai-proxy:8000}
      # OCR Engine configuration
      OCR_ENGINE: ${OCR_ENGINE:-paddleocr}
      MAX_FILE_SIZE: ${OCR_MAX_FILE_SIZE:-52428800}
      # Storage paths
      TEMP_DIR: /tmp/ocr
      # Development mode
      AUTH_MODE: "false"
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    volumes:
      - ocr_uploads:/tmp/ocr
      - ocr_models:/root/.paddleocr
    ports:
      - "8007:8007"
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8007/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - one-data-network

  # ============================================
  # Behavior Analysis Service
  # ============================================
  behavior-service:
    build:
      context: ../../services/behavior-service
      dockerfile: Dockerfile
    container_name: one-data-behavior-service
    environment:
      # Database configuration
      DATABASE_URL: mysql+pymysql://${MYSQL_USER:-onedata}:${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}@mysql:3306/${MYSQL_DATABASE:-onedata}
      # Redis configuration
      REDIS_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/1
      # Development mode
      AUTH_MODE: "false"
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    ports:
      - "8008:8008"
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8008/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - one-data-network

  web-frontend:
    build:
      context: ../../web
      dockerfile: Dockerfile
      args:
        VITE_AUTH_MODE: keycloak
        VITE_KEYCLOAK_URL: http://localhost:8080
        VITE_KEYCLOAK_REALM: one-data-studio
        VITE_KEYCLOAK_CLIENT_ID: web-frontend
        VITE_ENABLE_MOCK_LOGIN: false
    container_name: one-data-web
    environment:
      VITE_API_BASE_URL: http://localhost:8000
    ports:
      - "3000:80"
    depends_on:
      agent-api:
        condition: service_started
      data-api:
        condition: service_started
      openai-proxy:
        condition: service_started
      admin-api:
        condition: service_started
      model-api:
        condition: service_started
      keycloak:
        condition: service_started
    networks:
      - one-data-network

  # ============================================
  # Workflow Scheduling (Apache DolphinScheduler)
  # ============================================
  zookeeper:
    image: zookeeper:3.8.0
    container_name: one-data-zookeeper
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=0.0.0.0:2888:3888;2181
      ZOO_TICK_TIME: 2000
      ZOO_INIT_LIMIT: 10
      ZOO_SYNC_LIMIT: 5
      ALLOW_ANONYMOUS_LOGIN: "yes"
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/data
      - zookeeper_datalog:/datalog
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - one-data-network

  dolphinscheduler-postgresql:
    image: postgres:15.4-alpine
    container_name: one-data-dolphinscheduler-postgresql
    environment:
      POSTGRES_USER: ${DOLPHINSCHEDULER_DB_USER:-dolphinscheduler}
      POSTGRES_PASSWORD: ${DOLPHINSCHEDULER_DB_PASSWORD:?DOLPHINSCHEDULER_DB_PASSWORD must be set}
      POSTGRES_DB: dolphinscheduler
    ports:
      - "5433:5432"
    volumes:
      - dolphinscheduler_pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DOLPHINSCHEDULER_DB_USER:-dolphinscheduler}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - one-data-network

  dolphinscheduler-api:
    image: apache/dolphinscheduler-standalone-server:3.2.0
    container_name: one-data-dolphinscheduler
    environment:
      DATABASE: postgresql
      DATABASE_DRIVER: org.postgresql.Driver
      DATABASE_HOST: dolphinscheduler-postgresql
      DATABASE_PORT: 5432
      DATABASE_DB: dolphinscheduler
      DATABASE_USER: ${DOLPHINSCHEDULER_DB_USER:-dolphinscheduler}
      DATABASE_PASSWORD: ${DOLPHINSCHEDULER_DB_PASSWORD:?DOLPHINSCHEDULER_DB_PASSWORD must be set}
      ZOOKEEPER_QUORUM: zookeeper:2181
      # Registry configuration
      REGISTRY_TYPE: zookeeper
      REGISTRY_ZOOKEEPER_CONNECT_STRING: zookeeper:2181
      # API Server configuration
      API_SERVER_PORT: 12345
      # Master configuration
      MASTER_EXEC_THREADS: 100
      MASTER_EXEC_TASK_NUM: 20
      MASTER_HEARTBEAT_INTERVAL: PT10S
      MASTER_TASK_COMMIT_RETRYTIMES: 5
      MASTER_MAX_CPULOAD_AVG: 10
      MASTER_RESERVED_MEMORY: 0.3
      # Worker configuration
      WORKER_EXEC_THREADS: 100
      WORKER_HEARTBEAT_INTERVAL: PT10S
      WORKER_MAX_CPULOAD_AVG: 10
      WORKER_RESERVED_MEMORY: 0.3
      WORKER_TASK_PERSISTENT_WAITING_TIMEOUT_MINUTES: 1
      # Alert configuration
      ALERT_PORT: 50052
    ports:
      - "12345:12345"  # API Server
      - "25333:25333"  # Master
      - "25334:25334"  # Worker (if standalone)
      - "50052:50052"  # Alert
    volumes:
      - dolphinscheduler_logs:/opt/dolphinscheduler/logs
      - dolphinscheduler_shared:/opt/dolphinscheduler/shared
      - dolphinscheduler_resources:/opt/dolphinscheduler/resources
    depends_on:
      zookeeper:
        condition: service_healthy
      dolphinscheduler-postgresql:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:12345/dolphinscheduler/auth/login || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - one-data-network

  # ============================================
  # BI Analytics (Apache Superset)
  # ============================================
  superset-cache:
    image: redis:7-alpine
    container_name: one-data-superset-cache
    command: redis-server --requirepass ${REDIS_PASSWORD:?REDIS_PASSWORD must be set}
    ports:
      - "6380:6379"
    volumes:
      - superset_cache_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - one-data-network

  superset:
    image: apache/superset:3.1.0
    container_name: one-data-superset
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY:?SUPERSET_SECRET_KEY must be set}
      SUPERSET_LOAD_EXAMPLES: "no"
      # Database configuration
      DATABASE_DIALECT: mysql
      DATABASE_HOST: mysql
      DATABASE_PORT: 3306
      DATABASE_DB: superset_db
      DATABASE_USER: ${MYSQL_USER:-onedata}
      DATABASE_PASSWORD: ${MYSQL_PASSWORD:?MYSQL_PASSWORD must be set}
      # Redis cache configuration
      REDIS_HOST: superset-cache
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:?REDIS_PASSWORD must be set}
      REDIS_CELERY_DB: 0
      REDIS_CACHE_DB: 1
      # Webserver configuration
      SUPERSET_WEBSERVER_PORT: 8088
      SUPERSET_WEBSERVER_TIMEOUT: 120
      # Celery configuration
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@superset-cache:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@superset-cache:6379/1
      # Feature flags
      FEATURE_FLAGS: "ENABLE_PROXY_FIX=true"
      # Logging
      SUPERSET_LOG_LEVEL: INFO
    ports:
      - "8088:8088"
    volumes:
      - superset_home:/app/superset_home
      - superset_logs:/var/log/superset
    depends_on:
      mysql:
        condition: service_healthy
      superset-cache:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8088/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s
    networks:
      - one-data-network
    command: >
      bash -c "
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Admin --lastname User --email admin@onedata.local --password $${SUPERSET_ADMIN_PASSWORD:-admin123} &&
      superset init &&
      gunicorn --bind 0.0.0.0:8088 --access-logfile - --error-logfile - --workers 4 --worker-class gthread --threads 20 --timeout 120 'superset.app:create_app()'
      "

  # ============================================
  # Data Integration (Apache SeaTunnel)
  # ============================================
  seatunnel-zookeeper:
    image: zookeeper:3.8.0
    container_name: one-data-seatunnel-zk
    environment:
      ZOO_TICK_TIME: 2000
      ALLOW_ANONYMOUS_LOGIN: "yes"
    ports:
      - "2182:2181"
    volumes:
      - seatunnel_zk_data:/data
      - seatunnel_zk_datalog:/datalog
    networks:
      - one-data-network

  seatunnel:
    image: apache/seatunnel:2.3.12
    container_name: one-data-seatunnel
    environment:
      JAVA_OPTS: "-Xms2g -Xmx2g -Dseatunnel.defaults.jdbc.driver=com.mysql.cj.jdbc.Driver"
      HOSTNAME: "seatunnel"
    command: ["/opt/seatunnel/bin/seatunnel.sh", "--config", "/seatunnel/config/seatunnel.conf", "--check", "true"]
    ports:
      - "5801:5801"  # Web UI
    volumes:
      - ./config/seatunnel:/seatunnel/config:rw
      - seatunnel_data:/seatunnel/data
      - seatunnel_logs:/seatunnel/logs
      - seatunnel_shared:/seatunnel/shared
    networks:
      - one-data-network

  # ============================================
  # Monitoring (optional, use docker-compose.monitoring.yml)
  # ============================================

volumes:
  mysql_data:
  redis_data:
  minio_data:
  milvus_data:
  etcd_data:
  elasticsearch_data:
  vllm_cache:
  kettle_data:
  kettle_jdbc:
  ocr_uploads:
  ocr_models:
  # DolphinScheduler volumes
  zookeeper_data:
  zookeeper_datalog:
  dolphinscheduler_pg_data:
  dolphinscheduler_logs:
  dolphinscheduler_shared:
  dolphinscheduler_resources:
  # Superset volumes
  superset_cache_data:
  superset_home:
  superset_logs:
  # SeaTunnel volumes
  seatunnel_zk_data:
  seatunnel_zk_datalog:
  seatunnel_data:
  seatunnel_logs:
  seatunnel_shared:
  # Ollama volumes
  ollama_data:
  # Label Studio volumes
  label_studio_pg_data:
  label_studio_data:
  # Great Expectations volumes
  ge_data:
  # Apache Hop volumes
  hop_data:
  hop_config:
  # ShardingSphere volumes
  shardingsphere_conf:
  shardingsphere_ext:

networks:
  one-data-network:
    driver: bridge
